# -*- coding: utf-8 -*-
"""combine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pATDMZaVggKf6SQAdJ1BTjsop9W3tFC8

## Step 1: Prepare the data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
import seaborn as sns
import warnings
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from joblib import dump, load

# Read the dataset
data = pd.read_csv('SalaryData.csv')

data.head()

data.info()
data.isnull().sum()

data.describe()

#the number of occurrences of every Age
plt.figure(figsize=(20, 6))
sns.countplot(data=data,x='Age')
plt.show()

#the number of occurrences of every Years of Experience
plt.figure(figsize=(20, 6))
sns.countplot(data=data,x='Years of Experience')
plt.show()

#average salary for particular age
plt.figure(figsize=(20, 6))

sns.barplot(x='Age', y='Salary', data=data)

plt.title('Salary Distribution by Age')
plt.xlabel('Age')
plt.ylabel('Salary')
plt.show()

#average salary for particular year of experience
plt.figure(figsize=(20, 6))

sns.barplot(x='Years of Experience', y='Salary', data=data)

plt.title('Salary Distribution by Years of Experience')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

#displays the median (the central line inside the box) of the salary for each age group
plt.figure(figsize=(20, 6))

sns.boxplot(x='Age', y='Salary', data=data)

plt.title('Salary Accourding To Age')
plt.xlabel('Age')
plt.ylabel('Salary')
plt.show()

#displays the median (the central line inside the box) of the salary for each year of experience group
plt.figure(figsize=(20, 6))

sns.boxplot(x='Years of Experience', y='Salary', data=data)

plt.title('Salary Accourding To Years of Experience')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

"""### Data Preprocessing"""

# Impute missing numeric values (mean) and drop rows with missing categorical values
numeric_features = ['Age', 'Years of Experience']
categorical_features = ['Gender', 'Education Level', 'Job Title']

# Impute missing numeric values with mean
imputer = SimpleImputer(strategy='mean')
data[numeric_features] = imputer.fit_transform(data[numeric_features])

# Drop rows with missing categorical values (as the dataset is small)
data.dropna(subset=categorical_features, inplace=True)

# Drop rows where the target 'Salary' is missing
data.dropna(subset=['Salary'], inplace=True)
data.isnull().sum()

#drop salary in X, and put it as y
X = data.drop('Salary', axis=1)
y = data['Salary']
X

#encoding categorical variables and standardizing numerical variables
label_encoders = {}
scaler_columns={}

# Encode categorical variables using Label Encoding
for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])
    label_encoders[column] = le

# standardize numerical variable
for column in X.select_dtypes(include=['float64']).columns:
    scaler = StandardScaler()
    X[column] = scaler.fit_transform(X[[column]])
    scaler_columns[column] = scaler
X

# Print the encoding mapping for each categorical column
count = 0
for column, le in label_encoders.items():
    count = 0
    print(f"Encoding for column: {column}")
    for idx, class_ in enumerate(le.classes_):
        # print(f"  {class_}: {idx}")
        count+=1
    print(f"Count for column: {count} \n")

"""## Random Forest

### Step 2 & 3 : Feature Extraction and Split the data (into training and testing set)
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape)           #see all data shape (row, column), column needs to be same in train/test
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
print(X_test) #these data will be used to test the model
print(y_test)

"""### Step 4: Fit model and predict outcomes [Code]"""

# Define the Random Forest model
rf = RandomForestRegressor()

#train model with training dataset
rf.fit(X_train, y_train)

#test using train data (to find overfitting)
y_pred_train = rf.predict(X_train)
y_pred = rf.predict(X_test)

# Calculate Mean Absolute Error
mae = mean_absolute_error(y_train, y_pred_train)

# Calculate MAE as a percentage of the mean actual value
mae_percentage = (mae / y_test.mean()) * 100
print("Mean Absolute Error (MAE) Percentage:", mae_percentage, "%")

#find rmse of train model
mse = mean_squared_error(y_train, y_pred_train)
rmse = mse ** 0.5
print("Root Mean Squared Error (RMSE):", rmse)

# Calculate R-squared
r2 = r2_score(y_train, y_pred_train)
print("R-squared (R2):", r2 * 100, "%")

# Assuming 'X' is original DataFrame
feature_names = X.columns  # Store the column names before splitting

# Get feature importances from RandomForest model
importances = rf.feature_importances_

# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Create a DataFrame for easy plotting
features = X.columns
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(importance_df)

# Plot feature importances
plt.figure(figsize=(12, 6))
plt.title("Feature Importances")
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), feature_names[indices])
plt.show()

"""#### perform fine-tuning

##### Search the best cv value
"""

# List of possible `cv` values to test
cv_values = list(range(2, 20, 1))

# Store the average scores for each `cv` value
cv_scores = {}

# Loop through each `cv` value to test
for cv in cv_values:
    # Perform cross-validation
    scores = cross_val_score(rf, X, y, cv=cv, scoring='neg_mean_squared_error')

    # Calculate the mean score (use negative MSE because cross_val_score minimizes loss)
    mean_score = np.mean(scores)

    # Store the average score for this `cv` value
    cv_scores[cv] = mean_score
    print(f'CV={cv}: Mean Score={mean_score:.4f}')

# Find the `cv` with the best score
best_cv = max(cv_scores, key=cv_scores.get)
print(f'Best `cv` value: {best_cv} with Mean Score: {cv_scores[best_cv]:.4f}')

"""##### After using cv value for training data"""

# Function to peek at 10 random results
def peek_results(y_actual, y_pred, num=10):
    print('Random sample of results:')
    selected = np.random.randint(0, len(y_actual), num)
    for i in selected:
        print(f'Actual: {y_actual.iloc[i]} | Predicted: {y_pred[i]}')

# Generate cross-validated predictions using cross_val_predict
y_pred_cv = cross_val_predict(rf, X_train, y_train, cv=14)

# Peek at the results using the peek_results function
peek_results(y_train, y_pred_cv)

# Calculate MAE as a percentage of the mean actual value
mae_percentage = (mae / y_test.mean()) * 100
print("Mean Absolute Error (MAE) Percentage:", mae_percentage, "%")

# Evaluate the model using Mean Squared Error and Root Mean Squared Error
mse = mean_squared_error(y_train, y_pred_cv)
rmse = mse ** 0.5
print("Root Mean Squared Error (RMSE):", rmse)

# Calculate R-squared
r2 = r2_score(y_train, y_pred_cv)
print("R-squared (R2):", r2 * 100, "%")

"""##### Search the best random state"""

# Suppress FutureWarnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Set the range of random states to test
random_states = range(1, 101)  # Example range from 1 to 100

# Store RMSE results for each random state
rmse_results = []
r2_results = []
mae_results = []
adjusted_r2_results = []
mape_results = []
medae_results = []

for state in random_states:
    # Split the data with the current random state
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=state)

    # Initialize and train the model
    rf = RandomForestRegressor(
        random_state=state
    )
    rf.fit(X_train, y_train)

    # Predict on the test set
    y_pred_train = rf.predict(X_train)

    # Calculate MAE for the current random state
    mae = mean_absolute_error(y_train, y_pred_train)
    mae_results.append((state, mae))

    # Calculate RMSE for the current random state
    rmse = mean_squared_error(y_train, y_pred_train, squared=False)
    rmse_results.append((state, rmse))

    # Calculate RÂ² for the current random state
    r2 = r2_score(y_train, y_pred_train)
    r2_results.append((state, r2))

# Find the random state with the lowest MAE
best_state = min(mae_results, key=lambda x: x[1])
print(f"Best random_state: {best_state[0]} with MAE: {best_state[1]:.2f}")

# Find the random state with the lowest RMSE
best_state = min(rmse_results, key=lambda x: x[1])
print(f"Best random_state: {best_state[0]} with RMSE: {best_state[1]}")

# Find the random state with the highest R-squared
best_state = max(r2_results, key=lambda x: x[1])
print(f"Best random_state: {best_state[0]} with R-squared: {best_state[1] * 100:.2f}%")

"""##### Search the best parameter grid"""

# Define the parameter grid
param_grid = {
    'n_estimators': list(range(100, 500, 100)),  # Number of trees
    'min_samples_split': [2, 5, 10],  # Minimum samples to split a node
    'min_samples_leaf': [1, 2, 4],    # Minimum samples at a leaf node
    'max_depth': [None] + list(range(10, 100, 5)),  # Depth of trees
    'max_features': list(range(1, 50, 5)),  # Range from 1 to 50 with increment of 5
}

# param_grid = {
#     'n_estimators': [400],  # Number of trees
#     'min_samples_split': [2],  # Minimum samples to split a node
#     'min_samples_leaf': [1],    # Minimum samples at a leaf node
#     'max_depth': [10],  # Depth of trees
#     'max_features': [1],  # Range from 1 to 50 with increment of 5
# }

# Initialize the RandomForestRegressor
rf = RandomForestRegressor(random_state=60)

# Initialize GridSearchCV
grid_search = GridSearchCV(rf, param_grid, cv=14, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)

# Perform the grid search
print('Performing grid search...', end='')
grid_search.fit(X_train, y_train)
print('done')

# Output the best parameters
best_params = grid_search.best_params_
print("Best parameters found:", best_params)

# Retrain the model using the best parameters
best_forest_model = RandomForestRegressor(**best_params, random_state=13)
best_forest_model.fit(X_train, y_train)

# Predict on the test set using the tuned model
y_pred_best = best_forest_model.predict(X_test)

# Predict on the test set using the best model from GridSearchCV
y_pred_best = best_forest_model.predict(X_train)

# Calculate R-squared (RÂ²) for the best model
r2_best = r2_score(y_train, y_pred_best)
print("R-squared of the best model:", r2_best * 100, "%")

"""### Step 5: Evaluate the model [Code]

#### show prediction for test result before fine-tune
"""

# Calculate Mean Absolute Error
mae = mean_absolute_error(y_test, y_pred)

# Calculate MAE as a percentage of the mean actual value
mae_percentage = (mae / y_test.mean()) * 100
print("Mean Absolute Error (MAE) Percentage:", mae_percentage, "%")

# Evaluate the model using Mean Squared Error and Root Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
print("Root Mean Squared Error (RMSE):", rmse)

# Calculate R-squared
r2 = r2_score(y_test, y_pred)
print("R-squared (R2):", r2 * 100, "%")

"""#### after fine-tune"""

# Predict the target on the testing set
y_pred_test = best_forest_model.predict(X_test)

# Calculate Mean Absolute Error
mae = mean_absolute_error(y_test, y_pred_test)

# Calculate MAE as a percentage of the mean actual value
mae_percentage = (mae / y_test.mean()) * 100
print("Mean Absolute Error (MAE) Percentage:", mae_percentage, "%")

# Evaluate the model using Mean Squared Error and Root Mean Squared Error
mse = mean_squared_error(y_test, y_pred_test)
rmse = mse ** 0.5
print("Root Mean Squared Error (RMSE):", rmse)

# Calculate R-squared
r2 = r2_score(y_test, y_pred_test)
print("R-squared (R2):", r2 * 100, "%")

"""#### Comparison"""

# Results before fine-tuning
mae_before = mean_absolute_error(y_test, y_pred)
mae_percentage_before = (mae_before / y_test.mean()) * 100
mse_before = mean_squared_error(y_test, y_pred)
rmse_before = mse_before ** 0.5
r2_before = r2_score(y_test, y_pred) * 100

# Results after fine-tuning
mae_after_rf = mean_absolute_error(y_test, y_pred_test)
mae_percentage_after_rf = (mae_after_rf / y_test.mean()) * 100
mse_after_rf = mean_squared_error(y_test, y_pred_test)
rmse_after_rf = mse_after_rf ** 0.5
r2_after_rf = r2_score(y_test, y_pred_test) * 100

# Create a DataFrame to display the results
results = pd.DataFrame({
    'Metric': ['Mean Absolute Error (MAE)', 'MAE Percentage', 'Root Mean Squared Error (RMSE)', 'R-squared (R2)'],
    'Before Fine-Tuning': [mae_before, mae_percentage_before, rmse_before, r2_before],
    'After Fine-Tuning': [mae_after_rf, mae_percentage_after_rf, rmse_after_rf, r2_after_rf]
})

# Display the DataFrame
print(results.to_string(index=False))

"""### Step 6: Predict unseen data"""

# Sample test data

test_data = {
    'Age': [28],  # Age values
    'Gender': ['Female'],  # Gender values
    'Education Level': ['Master\'s'],  # Education levels
    'Job Title': ['Data Analyst'],  # Job titles
    'Years of Experience': [3]  # Years of experience
}

# Convert to DataFrame
test_df = pd.DataFrame(test_data)

# Preprocess unseen data
for column, le in label_encoders.items():
    if column in test_df.columns:
        test_df[column] = le.transform(test_df[column])
        # print(test_df[column])

for column, scaler in scaler_columns.items():
    # Ensure the column exists in the unseen data
    if column in test_df.columns:
        # Apply the scaler and update the column
        test_df[[column]] = scaler.transform(test_df[[column]])
        # print(test_df[column])

# Ensure all columns are present and in the correct order
test_df = test_df[X.columns]

# Predict the salary_in_usd for new unseen data
predicted_salary = best_forest_model.predict(test_df)

print(f"Predicted salary for unseen data: {predicted_salary[0]:.2f}")

"""### Step 7 Store the model in Joblib"""

dump(best_forest_model, 'best_forest_model.joblib')

# Load the model
# test_best_forest_model = load('best_forest_model.joblib')

# # Sample user input
# age = float(input("Enter Age: "))
# gender = input("Enter Gender (Male/Female): ")
# education_level = input("Enter Education Level (e.g., Bachelor's, Master's, etc.): ")
# job_title = input("Enter Job Title (e.g., Data Scientist, Data Engineer, etc.): ")
# years_of_experience = float(input("Enter Years of Experience: "))

# # Create a dictionary to represent the user input
# entered_data = {
#     'Age': [age],
#     'Gender': [gender],
#     'Education Level': [education_level],
#     'Job Title': [job_title],
#     'Years of Experience': [years_of_experience]
# }

# test_entered_data = pd.DataFrame(entered_data)

# test_label_encoders = {}

# for column in test_entered_data.select_dtypes(include=['object']).columns:
#     le = LabelEncoder()
#     test_entered_data[column] = le.fit_transform(test_entered_data[column])
#     test_label_encoders[column] = le

# # Initialize StandardScaler for numerical features
# test_scaler = StandardScaler()

# # Scale numerical variables
# for column in test_entered_data.select_dtypes(include=['float64']).columns:
#     test_entered_data[column] = scaler.fit_transform(test_entered_data[[column]])


# # Assuming 'rf' is your trained RandomForestRegressor model
# test_y_pred_test = test_best_forest_model.predict(test_entered_data)
# print("\nPredicted Salary:", round(test_y_pred_test[0], 2))

"""## Decision Tree

### Step 2 & 3 : Feature Extraction and Split the data (into training and testing set)
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape)           #see all data shape (row, column), column needs to be same in train/test
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""### Step 4: Fit model and predict outcomes [Code]

#### Get feature importances
"""

# Define the Decision Tree model
decisiontree = DecisionTreeRegressor(random_state=42)

# Fit the model
decisiontree.fit(X_train, y_train)

# Get feature importances
importances = decisiontree.feature_importances_

# Create a DataFrame for easy plotting
features = X.columns
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(importance_df)

plt.figure(figsize=(10,6))
plt.barh(features, importances)
plt.xlabel("Importance")
plt.title("Feature Importance for Decision Tree")
plt.show()

"""#### Train Model"""

# Define the Decision Tree model
decisiontree = DecisionTreeRegressor(random_state=42)

#train model with training dataset
decisiontree.fit(X_train, y_train)

#test using train data (to find overfitting)
y_pred_train_tree = decisiontree.predict(X_train)
y_pred = decisiontree.predict(X_test)

#find rmse of train model
mse = mean_squared_error(y_train, y_pred_train_tree)
rmse = mse ** 0.5
print("Root Mean Squared Error (RMSE):", rmse)

# Calculate R-squared
r2 = r2_score(y_train, y_pred_train_tree)
print("R-squared (R2):", r2 * 100, "%")

"""#### Show Random Prediction Result"""

# Function to peek at 10 random results
def peek_results(y_actual, y_pred, num=10):
    print('Random sample of results:')
    selected = np.random.randint(0, len(y_actual), num)
    for i in selected:
        print(f'Actual: {y_actual.iloc[i]} | Predicted: {y_pred[i]}')

# Generate cross-validated predictions using cross_val_predict
y_pred_cv_tree = cross_val_predict(decisiontree, X_train, y_train, cv=5)

# Peek at the results using the peek_results function
peek_results(y_train, y_pred_cv_tree)

"""#### Perform Fine-Tuning"""

# Perform parameter grid
param_grid = {
    'max_depth': [None] + list(range(5, 30, 5)),          # Depth of the tree (None for no limit or a range of values)
    'min_samples_split': list(range(2, 20, 2)),              # Minimum number of samples to split a node
    'min_samples_leaf': list(range(1, 10, 2)),                  # Minimum number of samples required in a leaf
    'criterion': ['squared_error', 'absolute_error'],             # Criterion for measuring the quality of a split
    'max_features': [None, 'sqrt', 'log2'],                       # Number of features to consider when looking for the best split
    'min_weight_fraction_leaf': [0.0, 0.01]          # Fraction of the total weight of samples required to be at a leaf node
}

# param_grid = {
#     'max_depth': [None],          # Depth of the tree (None for no limit or a range of values)
#     'min_samples_split': [10],              # Minimum number of samples to split a node
#     'min_samples_leaf': [1],                  # Minimum number of samples required in a leaf
#     'criterion': ['squared_error'],             # Criterion for measuring the quality of a split
#     'max_features': [None],                       # Number of features to consider when looking for the best split
#     'min_weight_fraction_leaf': [0.0]
# }

# Initialize GridSearchCV with DecisionTreeRegressor
grid_search = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)

# Perform the grid search
print("Performing grid search... Please wait.")
grid_search.fit(X_train, y_train)

# Check if any parameters were tuned
if grid_search.best_params_:
    print("\nGrid Search Complete!")
    print(f"Best parameters found: {grid_search.best_params_}")
else:
    print("\nNo hyperparameters were tuned. Using default parameters.")

# Output RMSE of the best model
best_rmse = np.sqrt(-grid_search.best_score_)
print(f"\nRMSE of the best model from Grid Search: {best_rmse:.4f}")

# Use the best model found by GridSearchCV
best_tree_model = grid_search.best_estimator_

# Predict on the training set using the best model
y_pred_train = best_tree_model.predict(X_train)

# Calculate R-squared (RÂ²) for the best model on the training set
r2_best_train = r2_score(y_train, y_pred_train)

# Display the R-squared score
print(f"Best Model Training R-squared: {r2_best_train * 100:.2f}%")

"""### Step 5: Evaluate the model [Code]

#### using fine-tuned model to predict test data
"""

# Predict the target on the testing set
y_pred_test = best_tree_model.predict(X_test)

# Evaluate the model using Mean Squared Error and Root Mean Squared Error
mse = mean_squared_error(y_test, y_pred_test)
rmse = mse ** 0.5

# Calculate R-squared
r2 = r2_score(y_test, y_pred_test)
print(f"Model Evaluation Results on Test Set:")
print(f"{'-'*40}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared: {r2 * 100:.2f}%")
print(f"{'-'*40}")

# Results before fine-tuning
mae_before = mean_absolute_error(y_test, y_pred)
mae_percentage_before = (mae_before / y_test.mean()) * 100
mse_before = mean_squared_error(y_test, y_pred)
rmse_before = mse_before ** 0.5
r2_before = r2_score(y_test, y_pred) * 100

# Results after fine-tuning
mae_after_dt = mean_absolute_error(y_test, y_pred_test)
mae_percentage_after_dt = (mae_after_dt / y_test.mean()) * 100
mse_after_dt = mean_squared_error(y_test, y_pred_test)
rmse_after_dt = mse_after_dt ** 0.5
r2_after_dt = r2_score(y_test, y_pred_test) * 100

# Create a DataFrame to display the results
results = pd.DataFrame({
    'Metric': ['Mean Absolute Error (MAE)', 'MAE Percentage', 'Root Mean Squared Error (RMSE)', 'R-squared (R2)'],
    'Before Fine-Tuning': [mae_before, mae_percentage_before, rmse_before, r2_before],
    'After Fine-Tuning': [mae_after_dt, mae_percentage_after_dt, rmse_after_dt, r2_after_dt]
})

# Display the DataFrame
print(results.to_string(index=False))

"""### Step 6: Predict unseen data"""

# Sample test data
test_data = {
    'Age': [25],                            # Age values
    'Gender': ['Female'],                   # Gender values
    'Education Level': ["Bachelor's"],      # Education levels
    'Job Title': ['Data Analyst'],          # Job titles
    'Years of Experience': [1]              # Years of experience
}

# Convert to DataFrame
test_df = pd.DataFrame(test_data)

# Preprocess unseen data
print("Preprocessing unseen data for prediction...\n")

# Label encode categorical variables
for column, le in label_encoders.items():
    if column in test_df.columns:
        test_df[column] = le.transform(test_df[column])
        print(f"Encoded {column}: {test_data[column][0]} -> {test_df[column][0]}")

# Standardize numerical variables
for column, scaler in scaler_columns.items():
    if column in test_df.columns:
        test_df[[column]] = scaler.transform(test_df[[column]])
        print(f"Standardized {column}: {test_df[column][0]}")

# Ensure all columns are present and in the correct order
test_df = test_df[X.columns]

# Predict the salary for the unseen data
predicted_salary = best_tree_model.predict(test_df)

# Display the prediction result
print("\nPrediction complete!")
print(f"Predicted Salary for the test data: ${predicted_salary[0]:,.2f}")

"""### Step 7 Store the model in Joblib"""

dump(best_tree_model, 'decision_tree_model.joblib')

# Load the model
# test_best_tree_model = load('decision_tree_model.joblib')

# # Sample user input
# age = float(input("Enter Age: "))
# gender = input("Enter Gender (Male/Female): ")
# education_level = input("Enter Education Level (e.g., Bachelor's, Master's, etc.): ")
# job_title = input("Enter Job Title (e.g., Data Scientist, Data Engineer, etc.): ")
# years_of_experience = float(input("Enter Years of Experience: "))

# # Create a dictionary to represent the user input
# entered_data = {
#     'Age': [age],
#     'Gender': [gender],
#     'Education Level': [education_level],
#     'Job Title': [job_title],
#     'Years of Experience': [years_of_experience]
# }

# test_entered_data = pd.DataFrame(entered_data)

# test_label_encoders = {}

# for column in test_entered_data.select_dtypes(include=['object']).columns:
#     le = LabelEncoder()
#     test_entered_data[column] = le.fit_transform(test_entered_data[column])
#     test_label_encoders[column] = le

# # Initialize StandardScaler for numerical features
# test_scaler = StandardScaler()

# # Scale numerical variables
# for column in test_entered_data.select_dtypes(include=['float64']).columns:
#     test_entered_data[column] = scaler.fit_transform(test_entered_data[[column]])


# test_y_pred_test = test_best_tree_model.predict(test_entered_data)
# print("\nPredicted Salary:", round(test_y_pred_test[0], 2))

"""## KNN

### Step 2 & 3 : Feature Extraction and Split the data (into training and testing set)
"""

# Splitting the Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### Step 4: Fit model and predict outcomes [Code]"""

# Initialize and train the KNN model with default parameters
knn_default = KNeighborsRegressor()
knn_default.fit(X_train, y_train)

# Predict on the test set
y_pred_default = knn_default.predict(X_test)
y_pred = knn_default.predict(X_test)

"""#### Predict data example"""

# Predict on a new data example
new_data = pd.DataFrame([[28, 'Female', "Master's", 3, 'Data Analyst']],
                        columns=['Age', 'Gender', 'Education Level', 'Years of Experience', 'Job Title'])

# Preprocess unseen data
for column, le in label_encoders.items():
    if column in new_data.columns:
        new_data[column] = le.transform(new_data[column])
        # print(new_data[column])

for column, scaler in scaler_columns.items():
    # Ensure the column exists in the unseen data
    if column in new_data.columns:
        # Apply the scaler and update the column
        new_data[[column]] = scaler.transform(new_data[[column]])
        # print(new_data[column])

# Ensure all columns are present and in the correct order
new_data = new_data[X.columns]

# Predict using the default model
predicted_salary_default = knn_default.predict(new_data)
print(f"Predicted Salary for new data using the default model: {predicted_salary_default[0]:.2f}")

"""#### Fine-Tuning"""

# 2. Fine-Tune the KNN Model Using GridSearchCV
# Define the parameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11, 13],  # Different values of n_neighbors to test
    'weights': ['uniform', 'distance'],   # Use uniform or distance-based weights
    'metric': ['euclidean', 'manhattan']  # Different distance metrics
}

# Initialize GridSearchCV
grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5, scoring='r2')

# Perform the grid search
grid_search.fit(X_train, y_train)

# Output the best parameters
best_params = grid_search.best_params_
print(f"Best Parameters: {best_params}")

"""### Step 5: Evaluate the model [Code]

#### Evaluate model before fine-tuning
"""

# Evaluate the model before fine-tuning
r2_default = r2_score(y_test, y_pred_default) * 100
mae_default = mean_absolute_error(y_test, y_pred_default)
mse_default = mean_squared_error(y_test, y_pred_default)

print("Before Fine-Tuning:")
print(f"Default Model R-squared: {r2_default}%")
print(f"Default Model Mean Absolute Error: {mae_default}")
print(f"Default Model Mean Squared Error: {mse_default}")

"""#### after fine-tune"""

# Predict on the test set using the best model from GridSearchCV
best_model = grid_search.best_estimator_
y_pred_test = best_model.predict(X_test)

# Calculate R-squared (RÂ²) for the best model
r2_best = r2_score(y_test, y_pred_test) * 100
mae_best = mean_absolute_error(y_test, y_pred_test)
mse_best = mean_squared_error(y_test, y_pred_test)

print(f"Best Model R-squared: {r2_best}%")
print(f"Best Model Mean Absolute Error: {mae_best}")
print(f"Best Model Mean Squared Error: {mse_best}")

"""#### Comparison"""

# Results before fine-tuning
mae_before = mean_absolute_error(y_test, y_pred)
mae_percentage_before = (mae_before / y_test.mean()) * 100
mse_before = mean_squared_error(y_test, y_pred)
rmse_before = mse_before ** 0.5
r2_before = r2_score(y_test, y_pred) * 100

# Results after fine-tuning
mae_after_knn = mean_absolute_error(y_test, y_pred_test)
mae_percentage_after_knn = (mae_after_knn / y_test.mean()) * 100
mse_after_knn = mean_squared_error(y_test, y_pred_test)
rmse_after_knn = mse_after_knn ** 0.5
r2_after_knn = r2_score(y_test, y_pred_test) * 100

# Create a DataFrame to display the results
results = pd.DataFrame({
    'Metric': ['Mean Absolute Error (MAE)', 'MAE Percentage', 'Root Mean Squared Error (RMSE)', 'R-squared (R2)'],
    'Before Fine-Tuning': [mae_before, mae_percentage_before, rmse_before, r2_before],
    'After Fine-Tuning': [mae_after_knn, mae_percentage_after_knn, rmse_after_knn, r2_after_knn]
})

# Display the DataFrame
print(results.to_string(index=False))

"""### Step 6: Predict unseen data"""

# Sample test data

test_data = {
    'Age': [28],  # Age values
    'Gender': ['Female'],  # Gender values
    'Education Level': ['Master\'s'],  # Education levels
    'Job Title': ['Data Analyst'],  # Job titles
    'Years of Experience': [3]  # Years of experience
}

# Convert to DataFrame
test_df = pd.DataFrame(test_data)

# Preprocess unseen data
for column, le in label_encoders.items():
    if column in test_df.columns:
        test_df[column] = le.transform(test_df[column])
        # print(test_df[column])

for column, scaler in scaler_columns.items():
    # Ensure the column exists in the unseen data
    if column in test_df.columns:
        # Apply the scaler and update the column
        test_df[[column]] = scaler.transform(test_df[[column]])
        # print(test_df[column])

# Ensure all columns are present and in the correct order
test_df = test_df[X.columns]

# Predict the salary_in_usd for new unseen data
predicted_salary = best_model.predict(test_df)

print(f"Predicted salary for unseen data: {predicted_salary[0]:.2f}")

"""### Step 7 Store the model in Joblib"""

dump(best_model, 'knn.joblib')

# Load the model
# test_best_model = load('knn.joblib')

# # Sample user input
# age = float(input("Enter Age: "))
# gender = input("Enter Gender (Male/Female): ")
# education_level = input("Enter Education Level (e.g., Bachelor's, Master's, etc.): ")
# job_title = input("Enter Job Title (e.g., Data Scientist, Data Engineer, etc.): ")
# years_of_experience = float(input("Enter Years of Experience: "))

# # Create a dictionary to represent the user input
# entered_data = {
#     'Age': [age],
#     'Gender': [gender],
#     'Education Level': [education_level],
#     'Job Title': [job_title],
#     'Years of Experience': [years_of_experience]
# }

# test_entered_data = pd.DataFrame(entered_data)

# test_label_encoders = {}

# for column in test_entered_data.select_dtypes(include=['object']).columns:
#     le = LabelEncoder()
#     test_entered_data[column] = le.fit_transform(test_entered_data[column])
#     test_label_encoders[column] = le

# # Initialize StandardScaler for numerical features
# test_scaler = StandardScaler()

# # Scale numerical variables
# for column in test_entered_data.select_dtypes(include=['float64']).columns:
#     test_entered_data[column] = scaler.fit_transform(test_entered_data[[column]])


# test_y_pred_test = best_model.predict(test_entered_data)
# print("\nPredicted Salary:", round(test_y_pred_test[0], 2))

"""## Analysis the result between 3 model"""

models = {
    'Random Forest': best_forest_model,
    'Decision Tree': best_tree_model,
    'KNN': best_model
}

results = {'Model': [], 'R-Mean Absolute Error (MAE)': [], 'MAE Percentage': [], 'Root Mean Squared Error (RMSE)': [], 'R-squared (R2)': []}

for i in models:
    obj = models[i]
    obj.fit(X_train, y_train)
    obj_pred = obj.predict(X_test)

    results = pd.DataFrame({
        'Model': ['Random Forest', 'Decision Tree', 'KNN'],
        'R-Mean Absolute Error (MAE)': [mae_after_rf, mae_after_dt, mae_after_knn],
        'MAE Percentage': [mae_percentage_after_rf, mae_percentage_after_dt, mae_percentage_after_knn],
        'Root Mean Squared Error (RMSE)': [rmse_after_rf, rmse_after_dt, rmse_after_knn],
        'R-squared (R2)': [r2_after_rf, r2_after_dt, r2_after_knn]
    })

# Create a DataFrame from the results dictionary
results_df = pd.DataFrame(results)

# Display the DataFrame without row numbering (index)
print(results_df.to_string(index=False))